{
  "comments": [
    {
      "key": {
        "uuid": "a1d469a6_26fba709",
        "filename": "datafile-app-server/src/main/java/org/onap/dcaegen2/collectors/datafile/tasks/ScheduledTasks.java",
        "patchSetId": 1
      },
      "lineNbr": 73,
      "author": {
        "id": 105
      },
      "writtenOn": "2019-02-27T13:41:33Z",
      "side": 1,
      "message": "should 10 be configurable?",
      "revId": "110cd14c4dd11aeeaa1788bf4305281be9ad7b0c",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "2558a7a5_7c23b557",
        "filename": "datafile-app-server/src/main/java/org/onap/dcaegen2/collectors/datafile/tasks/ScheduledTasks.java",
        "patchSetId": 1
      },
      "lineNbr": 103,
      "author": {
        "id": 2683
      },
      "writtenOn": "2019-02-25T11:04:28Z",
      "side": 1,
      "message": "I\u0027m not sure if this will work in case of error in the flux. doOnNext won\u0027t be called in such scenario and we can loose one \"virtual slot\" in the scheduler. You are handling some of these errors (handle*Error methods) but it won\u0027t be hard to introduce an invalid code in the future versions of this code.\n\nI can see a few solutions here:\n* Use Schedulers.newParallel(parallelism) instead of newElastic, if you think it\u0027s OK here. Then we can omit this manual counting altogether.\n* Maybe Reactor resource management (by means of Flux.using method) can be utilized here.\n* Decrement only in onSuccess and onError methods.\n* Use doFinally instead of manually decrementing the counter in every case. Will need to monitor for negative numbers as filter on shouldBePublished can remove some entries.",
      "revId": "110cd14c4dd11aeeaa1788bf4305281be9ad7b0c",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5ac6af4f_ff7027c2",
        "filename": "datafile-app-server/src/main/java/org/onap/dcaegen2/collectors/datafile/tasks/ScheduledTasks.java",
        "patchSetId": 1
      },
      "lineNbr": 103,
      "author": {
        "id": 5072
      },
      "writtenOn": "2019-02-25T13:01:04Z",
      "side": 1,
      "message": "It is true that onNext is not invoked when there is an error. \n\nThere are two other cases that can fail, which is handled by .onErrorResume (see collectFileFromXnf and publishToDataRouter). \nSo when the retrying of these tasks has given up, the counter is decreased and an empty flux is returned (by the onErrorResume handler), to remove with the next.\n\nIn these two cases the counter is also decreased (see handleCollectFailure resp. handlePublishFailure). \n\nAs far as I have been able to test so far this works fine. \n\nI was thinking about having the two .onErrorResume in this function to make that more clear but I figured that it was more readable to put the error handling in the collectFileFromXnf resp. publishToDataRouter functions instead.",
      "parentUuid": "2558a7a5_7c23b557",
      "revId": "110cd14c4dd11aeeaa1788bf4305281be9ad7b0c",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "b656c258_0c0f1273",
        "filename": "datafile-app-server/src/main/java/org/onap/dcaegen2/collectors/datafile/tasks/ScheduledTasks.java",
        "patchSetId": 1
      },
      "lineNbr": 103,
      "author": {
        "id": 5072
      },
      "writtenOn": "2019-02-25T13:09:29Z",
      "side": 1,
      "message": "I do not want to remove the counter. Its purpose is prevent an excessive queue of events to process in this process. \nAs long as this process has \"enough\" ongoing tasks, it will not fetch any new events (which is done by polling, in function consumeMessagesFromDmaap).",
      "parentUuid": "5ac6af4f_ff7027c2",
      "revId": "110cd14c4dd11aeeaa1788bf4305281be9ad7b0c",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "770742f9_e9b87836",
        "filename": "datafile-app-server/src/main/java/org/onap/dcaegen2/collectors/datafile/tasks/ScheduledTasks.java",
        "patchSetId": 1
      },
      "lineNbr": 103,
      "author": {
        "id": 2683
      },
      "writtenOn": "2019-02-26T06:56:39Z",
      "side": 1,
      "message": "My point is that it may work and looks pretty OK, but is not very maintainable. Someone in the future might not test it so thoroughly as you did.\n\nAfter rethinking, I came to another conclusion.\n\nIn rx you usually just subscribe to the events and reactor does stream \"throttling\" for you, keeping the threads busy. So you create a Flux emitting events consumed from DMaaP, then parallelize it, collect files and push back to DMaaP finally subscribing. Rx requests new events only if downstream subscribers can process them (backpressure mechanism). That is the precise problem which reactive streams tries to solve, see http://www.reactive-streams.org/ \"Problem\" section. So generally speaking, if done correctly (no blocking, single flow), the reactor framework should handle resource utilization for you. As an additional benefit we would get rid of this counting and (as a result) of handling error cases separately.\n\nWhat do you think? Can we refactor the code in this MR?",
      "parentUuid": "b656c258_0c0f1273",
      "revId": "110cd14c4dd11aeeaa1788bf4305281be9ad7b0c",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "a755c919_2541b243",
        "filename": "datafile-app-server/src/main/java/org/onap/dcaegen2/collectors/datafile/tasks/ScheduledTasks.java",
        "patchSetId": 1
      },
      "lineNbr": 182,
      "author": {
        "id": 3763
      },
      "writtenOn": "2019-02-22T10:09:52Z",
      "side": 1,
      "message": "Will this behaviour not loose messages? We have collected them from MR so we will not get them again. And we do not handle them here, so they will be lost. To not loose messages, the ones not handled here must be stored in some way and handled before the new ones collected from MR when there are threads available again.",
      "revId": "110cd14c4dd11aeeaa1788bf4305281be9ad7b0c",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "3001ae97_574a236b",
        "filename": "datafile-app-server/src/main/java/org/onap/dcaegen2/collectors/datafile/tasks/ScheduledTasks.java",
        "patchSetId": 1
      },
      "lineNbr": 182,
      "author": {
        "id": 5072
      },
      "writtenOn": "2019-02-22T10:45:03Z",
      "side": 1,
      "message": "No, this is where messages are fetched. So if the collector is \"too busy\" it will not fetch any new messages. \n\nI think the name of this and many other functions can be improved. I think we should try to get rid of aberiviations (like Dmaap, which does not add much value here).",
      "parentUuid": "a755c919_2541b243",
      "revId": "110cd14c4dd11aeeaa1788bf4305281be9ad7b0c",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "dd343190_ddbce408",
        "filename": "datafile-app-server/src/main/java/org/onap/dcaegen2/collectors/datafile/tasks/ScheduledTasks.java",
        "patchSetId": 1
      },
      "lineNbr": 182,
      "author": {
        "id": 3763
      },
      "writtenOn": "2019-02-22T11:16:58Z",
      "side": 1,
      "message": "Ah, yes I see that now.\nMaybe it would be clearer if the work is done in the if clause if the number of current tasks is lower than the maximum, instead of returning here?",
      "parentUuid": "3001ae97_574a236b",
      "revId": "110cd14c4dd11aeeaa1788bf4305281be9ad7b0c",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    }
  ]
}